{"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"x3amXy8zo9_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745968923624,"user_tz":240,"elapsed":220,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"a65538f1-bc99-4c37-c512-def105032e46"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr 29 23:22:03 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# Connet to google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhLFIYgIfi65","executionInfo":{"status":"ok","timestamp":1745968964804,"user_tz":240,"elapsed":17841,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"cae9643b-c0c8-43d0-8cfb-633fe63fa0b7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["!cd '/content/drive/MyDrive/CLIP_video_training'"],"metadata":{"id":"P0KkIxOvgDql","executionInfo":{"status":"ok","timestamp":1745968968267,"user_tz":240,"elapsed":511,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/sijiasiga/CLIP_video.git '/content/drive/MyDrive/CLIP_video_training/CLIP_video_5'\n","# !git clone https://github.com/sijiasiga/CLIP_video.git"],"metadata":{"id":"JkNb7yOMfdEs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745968980337,"user_tz":240,"elapsed":1709,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"25a21eaa-f42a-422d-e1dc-188c99c3d33b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/drive/MyDrive/CLIP_video_training/CLIP_video_5'...\n","remote: Enumerating objects: 230, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 230 (delta 29), reused 0 (delta 0), pack-reused 187 (from 1)\u001b[K\n","Receiving objects: 100% (230/230), 1.51 MiB | 22.70 MiB/s, done.\n","Resolving deltas: 100% (128/128), done.\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGfdhKIXipJr","outputId":"2990eec4-ef45-4aa0-fc50-73d219eb557f","executionInfo":{"status":"ok","timestamp":1745968990742,"user_tz":240,"elapsed":7842,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: conda: command not found\n","Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Collecting boto3\n","  Downloading boto3-1.38.5-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Collecting botocore<1.39.0,>=1.38.5 (from boto3)\n","  Downloading botocore-1.38.5-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.13.0,>=0.12.0 (from boto3)\n","  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading boto3-1.38.5-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.38.5-py3-none-any.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n","Successfully installed boto3-1.38.5 botocore-1.38.5 jmespath-1.0.1 s3transfer-0.12.0\n"]}],"source":["# From CLIP\n","!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n","!pip install ftfy regex tqdm\n","!pip install opencv-python boto3 requests pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAT-NYLMm22_"},"outputs":[],"source":["# # Comment if the file exits\n","# !wget -O /content/drive/MyDrive/Data/msrvtt_data.zip https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n","# !unzip /content/drive/MyDrive/Data/msrvtt_data.zip -d /content/drive/MyDrive/Data/"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uYqhvAKVpQD9","executionInfo":{"status":"ok","timestamp":1745968997087,"user_tz":240,"elapsed":8,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[],"source":["DATA_PATH = \"/content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data\"  # Directory of MSRVTT data\n","VIDEO_PATH = \"/content/drive/MyDrive/CLIP_video_training/Data/video\"    # Directory of MSRVTT raw video"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3U2GCKmL14G6","colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"status":"ok","timestamp":1745969002773,"user_tz":240,"elapsed":3300,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"df72e687-0546-4ac7-b71b-512bdd46a14d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size:  9000\n","Column names ['video_id']\n"]},{"output_type":"display_data","data":{"text/plain":["  video_id\n","0   video0\n","1   video1\n","2   video2\n","3   video3\n","4   video4"],"text/html":["\n","  <div id=\"df-cd329df0-217d-4a58-928e-9c92f4638c9b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>video0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>video1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd329df0-217d-4a58-928e-9c92f4638c9b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cd329df0-217d-4a58-928e-9c92f4638c9b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cd329df0-217d-4a58-928e-9c92f4638c9b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-1f548491-3c60-432f-9853-31b28576ed24\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f548491-3c60-432f-9853-31b28576ed24')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-1f548491-3c60-432f-9853-31b28576ed24 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"Subset CSV saved to:\\\", subset_path)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video1\",\n          \"video4\",\n          \"video2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Size of subset： 900\n"]},{"output_type":"display_data","data":{"text/plain":["    video_id\n","0  video8405\n","1  video1162\n","2   video582\n","3  video4081\n","4  video9139"],"text/html":["\n","  <div id=\"df-dc375587-11cb-4363-bc99-61d3ff529973\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>video8405</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>video1162</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video582</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video4081</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video9139</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc375587-11cb-4363-bc99-61d3ff529973')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dc375587-11cb-4363-bc99-61d3ff529973 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dc375587-11cb-4363-bc99-61d3ff529973');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-261cd583-0203-4643-9e0e-60b85b488cbd\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-261cd583-0203-4643-9e0e-60b85b488cbd')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-261cd583-0203-4643-9e0e-60b85b488cbd button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"Subset CSV saved to:\\\", subset_path)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video1162\",\n          \"video9139\",\n          \"video582\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Subset CSV saved to: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_train.subset.csv\n"]}],"source":["import pandas as pd\n","\n","# Load training dataset\n","train_path = f\"{DATA_PATH}/MSRVTT_train.9k.csv\"\n","df = pd.read_csv(train_path)\n","\n","print(\"Size: \", len(df))\n","print(\"Column names\", df.columns.tolist())\n","display(df.head())\n","\n","# Choose a subset\n","def sample_subset(df, frac=0.1, random_state=42):\n","    return df.sample(frac=frac, random_state=random_state).reset_index(drop=True)\n","\n","# Set fraction to 0.01\n","subset_df = sample_subset(df, frac=0.1)\n","print(\"Size of subset：\", len(subset_df))\n","display(subset_df.head())\n","\n","# Save the subset\n","subset_path = f\"{DATA_PATH}/MSRVTT_train.subset.csv\"\n","subset_df.to_csv(subset_path, index=False)\n","\n","print(\"Subset CSV saved to:\", subset_path)\n"]},{"cell_type":"markdown","metadata":{"id":"4reTc9BCrqJx"},"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9E80wSjrI7z","outputId":"c34ffcd4-8ed7-4f3d-f77c-5859af3dbc98","executionInfo":{"status":"ok","timestamp":1745969012245,"user_tz":240,"elapsed":3611,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Current device: NVIDIA A100-SXM4-40GB\n"]}],"source":["import torch\n","print(\"Current device:\", torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU only\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VyebJOr2s_8n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745969041618,"user_tz":240,"elapsed":3420,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"a32d3124-4d80-41bb-93de-c541ace5a216"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-29 23:23:58--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n","Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.253.59, 2620:1ec:bdf::59\n","Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.253.59|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 353976522 (338M) [application/octet-stream]\n","Saving to: ‘/content/drive/MyDrive/CLIP_video_training/CLIP_video_5/modules/ViT-B-32.pt’\n","\n","ViT-B-32.pt         100%[===================>] 337.58M   105MB/s    in 3.2s    \n","\n","2025-04-29 23:24:01 (105 MB/s) - ‘/content/drive/MyDrive/CLIP_video_training/CLIP_video_5/modules/ViT-B-32.pt’ saved [353976522/353976522]\n","\n"]}],"source":["!wget -P /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/modules https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"6VDrwhRKsqgm","executionInfo":{"status":"ok","timestamp":1745969120515,"user_tz":240,"elapsed":7,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fp8YAC93w3z7"},"outputs":[],"source":["# # Updata changes to files. Comment if the files are not changed\n","# !cd CLIP_video && git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIV0xcbGPydW"},"outputs":[],"source":["# !pip install ffmpeg\n","# !python /content/CLIP_video/preprocess/compress_video.py --input_root \"/content/drive/MyDrive/IDL/IDL Project/Data/video\" --output_root \"/content/drive/MyDrive/IDL/IDL Project/Data/compressed_video\""]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kb8sD1HGuYxE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c27e838b-6605-47a3-87e5-7b26846b4da5","executionInfo":{"status":"ok","timestamp":1745995203551,"user_tz":240,"elapsed":26047714,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["04/29/2025 23:26:03 - INFO -   Effective parameters:\n","04/29/2025 23:26:03 - INFO -     <<< batch_size: 64\n","04/29/2025 23:26:03 - INFO -     <<< batch_size_val: 8\n","04/29/2025 23:26:03 - INFO -     <<< cache_dir: \n","04/29/2025 23:26:03 - INFO -     <<< coef_lr: 0.001\n","04/29/2025 23:26:03 - INFO -     <<< cross_model: cross-base\n","04/29/2025 23:26:03 - INFO -     <<< cross_num_hidden_layers: 4\n","04/29/2025 23:26:03 - INFO -     <<< data_path: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_data.json\n","04/29/2025 23:26:03 - INFO -     <<< datatype: msrvtt\n","04/29/2025 23:26:03 - INFO -     <<< do_eval: False\n","04/29/2025 23:26:03 - INFO -     <<< do_lower_case: False\n","04/29/2025 23:26:03 - INFO -     <<< do_pretrain: False\n","04/29/2025 23:26:03 - INFO -     <<< do_train: True\n","04/29/2025 23:26:03 - INFO -     <<< epochs: 20\n","04/29/2025 23:26:03 - INFO -     <<< eval_frame_order: 0\n","04/29/2025 23:26:03 - INFO -     <<< expand_msrvtt_sentences: True\n","04/29/2025 23:26:03 - INFO -     <<< feature_framerate: 1\n","04/29/2025 23:26:03 - INFO -     <<< features_path: /content/drive/MyDrive/CLIP_video_training/Data/video\n","04/29/2025 23:26:03 - INFO -     <<< fp16: True\n","04/29/2025 23:26:03 - INFO -     <<< fp16_opt_level: O1\n","04/29/2025 23:26:03 - INFO -     <<< freeze_layer_num: 6\n","04/29/2025 23:26:03 - INFO -     <<< gradient_accumulation_steps: 1\n","04/29/2025 23:26:03 - INFO -     <<< hard_negative_rate: 0.5\n","04/29/2025 23:26:03 - INFO -     <<< init_model: None\n","04/29/2025 23:26:03 - INFO -     <<< linear_patch: 2d\n","04/29/2025 23:26:03 - INFO -     <<< loose_type: True\n","04/29/2025 23:26:03 - INFO -     <<< lr: 0.0001\n","04/29/2025 23:26:03 - INFO -     <<< lr_decay: 0.9\n","04/29/2025 23:26:03 - INFO -     <<< margin: 0.1\n","04/29/2025 23:26:03 - INFO -     <<< max_frames: 8\n","04/29/2025 23:26:03 - INFO -     <<< max_words: 32\n","04/29/2025 23:26:03 - INFO -     <<< n_display: 100\n","04/29/2025 23:26:03 - INFO -     <<< n_gpu: 1\n","04/29/2025 23:26:03 - INFO -     <<< n_pair: 1\n","04/29/2025 23:26:03 - INFO -     <<< negative_weighting: 1\n","04/29/2025 23:26:03 - INFO -     <<< num_thread_reader: 8\n","04/29/2025 23:26:03 - INFO -     <<< output_dir: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType\n","04/29/2025 23:26:03 - INFO -     <<< pretrained_clip_name: ViT-B/32\n","04/29/2025 23:26:03 - INFO -     <<< rank: 0\n","04/29/2025 23:26:03 - INFO -     <<< resume_model: None\n","04/29/2025 23:26:03 - INFO -     <<< sampled_use_mil: False\n","04/29/2025 23:26:03 - INFO -     <<< seed: 42\n","04/29/2025 23:26:03 - INFO -     <<< sim_header: meanP\n","04/29/2025 23:26:03 - INFO -     <<< slice_framepos: 5\n","04/29/2025 23:26:03 - INFO -     <<< task_type: retrieval\n","04/29/2025 23:26:03 - INFO -     <<< text_num_hidden_layers: 12\n","04/29/2025 23:26:03 - INFO -     <<< train_csv: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_train.subset.csv\n","04/29/2025 23:26:03 - INFO -     <<< train_frame_order: 0\n","04/29/2025 23:26:03 - INFO -     <<< use_mil: False\n","04/29/2025 23:26:03 - INFO -     <<< val_csv: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_JSFUSION_test.csv\n","04/29/2025 23:26:03 - INFO -     <<< video_dim: 1024\n","04/29/2025 23:26:03 - INFO -     <<< visual_num_hidden_layers: 12\n","04/29/2025 23:26:03 - INFO -     <<< warmup_proportion: 0.1\n","04/29/2025 23:26:03 - INFO -     <<< world_size: 1\n","04/29/2025 23:26:03 - INFO -   device: cuda n_gpu: 1\n","04/29/2025 23:26:04 - INFO -   loading archive file /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/modules/cross-base\n","04/29/2025 23:26:04 - INFO -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"max_position_embeddings\": 128,\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 512\n","}\n","\n","04/29/2025 23:26:04 - INFO -   Weight doesn't exsits. /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/modules/cross-base/cross_pytorch_model.bin\n","04/29/2025 23:26:04 - WARNING -   Stage-One:True, Stage-Two:False\n","04/29/2025 23:26:04 - WARNING -   Test retrieval by loose type.\n","04/29/2025 23:26:04 - WARNING -   \t embed_dim: 512\n","04/29/2025 23:26:04 - WARNING -   \t image_resolution: 224\n","04/29/2025 23:26:04 - WARNING -   \t vision_layers: 12\n","04/29/2025 23:26:04 - WARNING -   \t vision_width: 768\n","04/29/2025 23:26:04 - WARNING -   \t vision_patch_size: 32\n","04/29/2025 23:26:04 - WARNING -   \t context_length: 77\n","04/29/2025 23:26:04 - WARNING -   \t vocab_size: 49408\n","04/29/2025 23:26:04 - WARNING -   \t transformer_width: 512\n","04/29/2025 23:26:04 - WARNING -   \t transformer_heads: 8\n","04/29/2025 23:26:04 - WARNING -   \t transformer_layers: 12\n","04/29/2025 23:26:04 - WARNING -   \t\t linear_patch: 2d\n","04/29/2025 23:26:04 - WARNING -   \t cut_top_layer: 0\n","04/29/2025 23:26:06 - WARNING -   \t sim_header: meanP\n","04/29/2025 23:26:10 - INFO -   --------------------\n","04/29/2025 23:26:10 - INFO -   Weights from pretrained model not used in CLIP4Clip: \n","   clip.input_resolution\n","   clip.context_length\n","   clip.vocab_size\n","04/29/2025 23:26:11 - INFO -   ***** Running test *****\n","04/29/2025 23:26:11 - INFO -     Num examples = 1000\n","04/29/2025 23:26:11 - INFO -     Batch size = 8\n","04/29/2025 23:26:11 - INFO -     Num steps = 125\n","04/29/2025 23:26:11 - INFO -   ***** Running val *****\n","04/29/2025 23:26:11 - INFO -     Num examples = 1000\n","04/29/2025 23:26:15 - INFO -   ***** Running training *****\n","04/29/2025 23:26:15 - INFO -     Num examples = 18000\n","04/29/2025 23:26:15 - INFO -     Batch size = 64\n","04/29/2025 23:26:15 - INFO -     Num steps = 5620\n","Epoch 1:  35% 99/281 [08:34<21:07,  6.96s/it, loss=1.09]04/29/2025 23:34:50 - INFO -   Epoch: 1/20, Step: 100/281, Lr: , Loss: 1.085496, Time/step: 5.148277\n","Epoch 1:  71% 199/281 [15:33<02:49,  2.06s/it, loss=0.811]04/29/2025 23:41:49 - INFO -   Epoch: 1/20, Step: 200/281, Lr: , Loss: 0.810746, Time/step: 4.190482\n","Epoch 1: 100% 281/281 [20:56<00:00,  4.47s/it, loss=0.733]\n","04/29/2025 23:47:12 - INFO -   Epoch 1/20 Finished, Train Loss: 1.175354\n","04/29/2025 23:47:13 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0\n","04/29/2025 23:47:13 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.0\n","Evaluating: 100% 125/125 [02:39<00:00,  1.28s/it]\n","04/29/2025 23:49:59 - INFO -   sim matrix size: 1000, 1000\n","04/29/2025 23:49:59 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/29/2025 23:49:59 - INFO -   Text-to-Video:\n","04/29/2025 23:49:59 - INFO -   \t>>>  R@1: 36.2 - R@5: 62.2 - R@10: 73.5 - Median R: 3.0 - Mean R: 21.2\n","04/29/2025 23:49:59 - INFO -   Video-to-Text:\n","04/29/2025 23:49:59 - INFO -   \t>>>  V2T$R@1: 37.1 - V2T$R@5: 64.0 - V2T$R@10: 75.0 - V2T$Median R: 3.0 - V2T$Mean R: 18.6\n","04/29/2025 23:49:59 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0, the R1 is: 36.2000\n","Epoch 2:   6% 18/281 [01:42<29:38,  6.76s/it, loss=0.918]04/29/2025 23:51:42 - INFO -   Epoch: 2/20, Step: 19/281, Lr: , Loss: 0.917712, Time/step: 1.031246\n","Epoch 2:  42% 118/281 [08:46<06:37,  2.44s/it, loss=0.854]04/29/2025 23:58:46 - INFO -   Epoch: 2/20, Step: 119/281, Lr: , Loss: 0.854018, Time/step: 4.238518\n","Epoch 2:  78% 218/281 [16:05<05:47,  5.51s/it, loss=0.533]04/30/2025 00:06:04 - INFO -   Epoch: 2/20, Step: 219/281, Lr: , Loss: 0.533531, Time/step: 4.384167\n","Epoch 2: 100% 281/281 [20:09<00:00,  4.30s/it, loss=0.711]\n","04/30/2025 00:10:09 - INFO -   Epoch 2/20 Finished, Train Loss: 0.717335\n","04/30/2025 00:10:10 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1\n","04/30/2025 00:10:10 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.1\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 00:11:28 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:11:28 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:11:28 - INFO -   Text-to-Video:\n","04/30/2025 00:11:28 - INFO -   \t>>>  R@1: 37.7 - R@5: 62.9 - R@10: 74.1 - Median R: 3.0 - Mean R: 20.1\n","04/30/2025 00:11:28 - INFO -   Video-to-Text:\n","04/30/2025 00:11:28 - INFO -   \t>>>  V2T$R@1: 37.0 - V2T$R@5: 66.1 - V2T$R@10: 76.8 - V2T$Median R: 3.0 - V2T$Mean R: 17.0\n","04/30/2025 00:11:28 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 37.7000\n","Epoch 3:  13% 37/281 [03:00<13:17,  3.27s/it, loss=0.365]04/30/2025 00:14:28 - INFO -   Epoch: 3/20, Step: 38/281, Lr: , Loss: 0.364858, Time/step: 1.805876\n","Epoch 3:  49% 137/281 [10:23<10:52,  4.53s/it, loss=0.451]04/30/2025 00:21:51 - INFO -   Epoch: 3/20, Step: 138/281, Lr: , Loss: 0.450800, Time/step: 4.429908\n","Epoch 3:  84% 237/281 [17:26<03:18,  4.50s/it, loss=0.335]04/30/2025 00:28:54 - INFO -   Epoch: 3/20, Step: 238/281, Lr: , Loss: 0.334650, Time/step: 4.231299\n","Epoch 3: 100% 281/281 [20:09<00:00,  4.31s/it, loss=0.435]\n","04/30/2025 00:31:38 - INFO -   Epoch 3/20 Finished, Train Loss: 0.490089\n","04/30/2025 00:31:39 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2\n","04/30/2025 00:31:39 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.2\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 00:32:56 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:32:56 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:32:56 - INFO -   Text-to-Video:\n","04/30/2025 00:32:56 - INFO -   \t>>>  R@1: 36.7 - R@5: 63.2 - R@10: 75.0 - Median R: 3.0 - Mean R: 21.0\n","04/30/2025 00:32:56 - INFO -   Video-to-Text:\n","04/30/2025 00:32:56 - INFO -   \t>>>  V2T$R@1: 37.2 - V2T$R@5: 64.4 - V2T$R@10: 76.0 - V2T$Median R: 3.0 - V2T$Mean R: 16.3\n","04/30/2025 00:32:56 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 37.7000\n","Epoch 4:  20% 56/281 [04:43<08:20,  2.23s/it, loss=0.265]04/30/2025 00:37:40 - INFO -   Epoch: 4/20, Step: 57/281, Lr: , Loss: 0.264720, Time/step: 2.836616\n","Epoch 4:  56% 156/281 [11:41<07:53,  3.78s/it, loss=0.333]04/30/2025 00:44:39 - INFO -   Epoch: 4/20, Step: 157/281, Lr: , Loss: 0.333004, Time/step: 4.184596\n","Epoch 4:  91% 256/281 [19:02<00:44,  1.79s/it, loss=0.29]04/30/2025 00:51:59 - INFO -   Epoch: 4/20, Step: 257/281, Lr: , Loss: 0.289642, Time/step: 4.407194\n","Epoch 4: 100% 281/281 [20:10<00:00,  4.31s/it, loss=0.377]\n","04/30/2025 00:53:08 - INFO -   Epoch 4/20 Finished, Train Loss: 0.355747\n","04/30/2025 00:53:09 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3\n","04/30/2025 00:53:09 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.3\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/30/2025 00:54:28 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:54:28 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:54:28 - INFO -   Text-to-Video:\n","04/30/2025 00:54:28 - INFO -   \t>>>  R@1: 38.2 - R@5: 62.5 - R@10: 74.5 - Median R: 3.0 - Mean R: 22.0\n","04/30/2025 00:54:28 - INFO -   Video-to-Text:\n","04/30/2025 00:54:28 - INFO -   \t>>>  V2T$R@1: 35.4 - V2T$R@5: 63.0 - V2T$R@10: 74.1 - V2T$Median R: 3.0 - V2T$Mean R: 17.8\n","04/30/2025 00:54:28 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 5:  27% 75/281 [05:53<22:26,  6.54s/it, loss=0.266]04/30/2025 01:00:22 - INFO -   Epoch: 5/20, Step: 76/281, Lr: , Loss: 0.265981, Time/step: 3.536469\n","Epoch 5:  62% 175/281 [12:56<04:38,  2.63s/it, loss=0.257]04/30/2025 01:07:25 - INFO -   Epoch: 5/20, Step: 176/281, Lr: , Loss: 0.256554, Time/step: 4.230499\n","Epoch 5:  98% 275/281 [20:04<00:30,  5.14s/it, loss=0.216]04/30/2025 01:14:33 - INFO -   Epoch: 5/20, Step: 276/281, Lr: , Loss: 0.215628, Time/step: 4.283618\n","Epoch 5: 100% 281/281 [20:08<00:00,  4.30s/it, loss=0.209]\n","04/30/2025 01:14:37 - INFO -   Epoch 5/20 Finished, Train Loss: 0.268853\n","04/30/2025 01:14:38 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4\n","04/30/2025 01:14:38 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.4\n","Evaluating: 100% 125/125 [01:12<00:00,  1.74it/s]\n","04/30/2025 01:15:56 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:15:56 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:15:56 - INFO -   Text-to-Video:\n","04/30/2025 01:15:56 - INFO -   \t>>>  R@1: 38.0 - R@5: 63.3 - R@10: 74.3 - Median R: 3.0 - Mean R: 21.9\n","04/30/2025 01:15:56 - INFO -   Video-to-Text:\n","04/30/2025 01:15:56 - INFO -   \t>>>  V2T$R@1: 36.3 - V2T$R@5: 62.7 - V2T$R@10: 74.3 - V2T$Median R: 3.0 - V2T$Mean R: 18.4\n","04/30/2025 01:15:56 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 6:  33% 94/281 [07:06<06:50,  2.20s/it, loss=0.18] 04/30/2025 01:23:03 - INFO -   Epoch: 6/20, Step: 95/281, Lr: , Loss: 0.179906, Time/step: 4.261896\n","Epoch 6:  69% 194/281 [14:38<10:24,  7.17s/it, loss=0.222]04/30/2025 01:30:35 - INFO -   Epoch: 6/20, Step: 195/281, Lr: , Loss: 0.221570, Time/step: 4.526895\n","Epoch 6: 100% 281/281 [20:13<00:00,  4.32s/it, loss=0.136]\n","04/30/2025 01:36:10 - INFO -   Epoch 6/20 Finished, Train Loss: 0.219049\n","04/30/2025 01:36:11 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.5\n","04/30/2025 01:36:11 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.5\n","Evaluating: 100% 125/125 [01:11<00:00,  1.76it/s]\n","04/30/2025 01:37:29 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:37:29 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:37:29 - INFO -   Text-to-Video:\n","04/30/2025 01:37:29 - INFO -   \t>>>  R@1: 37.3 - R@5: 61.6 - R@10: 73.8 - Median R: 3.0 - Mean R: 22.7\n","04/30/2025 01:37:29 - INFO -   Video-to-Text:\n","04/30/2025 01:37:29 - INFO -   \t>>>  V2T$R@1: 35.7 - V2T$R@5: 61.7 - V2T$R@10: 73.3 - V2T$Median R: 3.0 - V2T$Mean R: 18.9\n","04/30/2025 01:37:29 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 7:   5% 13/281 [01:17<17:03,  3.82s/it, loss=0.0976]04/30/2025 01:38:46 - INFO -   Epoch: 7/20, Step: 14/281, Lr: , Loss: 0.097564, Time/step: 0.774940\n","Epoch 7:  40% 113/281 [08:33<21:29,  7.68s/it, loss=0.138]04/30/2025 01:46:02 - INFO -   Epoch: 7/20, Step: 114/281, Lr: , Loss: 0.137920, Time/step: 4.360089\n","Epoch 7:  76% 213/281 [15:33<04:29,  3.96s/it, loss=0.266]04/30/2025 01:53:03 - INFO -   Epoch: 7/20, Step: 214/281, Lr: , Loss: 0.266097, Time/step: 4.204288\n","Epoch 7: 100% 281/281 [20:10<00:00,  4.31s/it, loss=0.177]\n","04/30/2025 01:57:39 - INFO -   Epoch 7/20 Finished, Train Loss: 0.190398\n","04/30/2025 01:57:41 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.6\n","04/30/2025 01:57:41 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.6\n","Evaluating: 100% 125/125 [01:11<00:00,  1.76it/s]\n","04/30/2025 01:58:58 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:58:58 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:58:58 - INFO -   Text-to-Video:\n","04/30/2025 01:58:58 - INFO -   \t>>>  R@1: 37.0 - R@5: 63.2 - R@10: 73.5 - Median R: 3.0 - Mean R: 22.4\n","04/30/2025 01:58:58 - INFO -   Video-to-Text:\n","04/30/2025 01:58:58 - INFO -   \t>>>  V2T$R@1: 35.6 - V2T$R@5: 62.3 - V2T$R@10: 72.6 - V2T$Median R: 3.0 - V2T$Mean R: 18.5\n","04/30/2025 01:58:58 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 8:  11% 32/281 [02:52<06:46,  1.63s/it, loss=0.158]04/30/2025 02:01:50 - INFO -   Epoch: 8/20, Step: 33/281, Lr: , Loss: 0.158007, Time/step: 1.722757\n","Epoch 8:  47% 132/281 [10:08<20:17,  8.17s/it, loss=0.113]04/30/2025 02:09:07 - INFO -   Epoch: 8/20, Step: 133/281, Lr: , Loss: 0.112700, Time/step: 4.362282\n","Epoch 8:  83% 232/281 [17:21<02:21,  2.88s/it, loss=0.101]04/30/2025 02:16:20 - INFO -   Epoch: 8/20, Step: 233/281, Lr: , Loss: 0.101075, Time/step: 4.335225\n","Epoch 8: 100% 281/281 [20:15<00:00,  4.33s/it, loss=0.162]\n","04/30/2025 02:19:14 - INFO -   Epoch 8/20 Finished, Train Loss: 0.163754\n","04/30/2025 02:19:15 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.7\n","04/30/2025 02:19:15 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.7\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 02:20:33 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 02:20:33 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 02:20:33 - INFO -   Text-to-Video:\n","04/30/2025 02:20:33 - INFO -   \t>>>  R@1: 36.1 - R@5: 63.3 - R@10: 74.2 - Median R: 3.0 - Mean R: 22.5\n","04/30/2025 02:20:33 - INFO -   Video-to-Text:\n","04/30/2025 02:20:33 - INFO -   \t>>>  V2T$R@1: 34.8 - V2T$R@5: 61.5 - V2T$R@10: 71.6 - V2T$Median R: 3.0 - V2T$Mean R: 19.0\n","04/30/2025 02:20:33 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 9:  18% 51/281 [04:08<24:00,  6.26s/it, loss=0.14]  04/30/2025 02:24:41 - INFO -   Epoch: 9/20, Step: 52/281, Lr: , Loss: 0.139488, Time/step: 2.488897\n","Epoch 9:  54% 151/281 [11:12<09:19,  4.30s/it, loss=0.113]04/30/2025 02:31:45 - INFO -   Epoch: 9/20, Step: 152/281, Lr: , Loss: 0.113222, Time/step: 4.239464\n","Epoch 9:  89% 251/281 [18:24<02:32,  5.07s/it, loss=0.0466]04/30/2025 02:38:57 - INFO -   Epoch: 9/20, Step: 252/281, Lr: , Loss: 0.046583, Time/step: 4.318460\n","Epoch 9: 100% 281/281 [20:08<00:00,  4.30s/it, loss=0.254]\n","04/30/2025 02:40:41 - INFO -   Epoch 9/20 Finished, Train Loss: 0.152708\n","04/30/2025 02:40:43 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8\n","04/30/2025 02:40:43 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.8\n","Evaluating: 100% 125/125 [01:10<00:00,  1.76it/s]\n","04/30/2025 02:42:00 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 02:42:00 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 02:42:00 - INFO -   Text-to-Video:\n","04/30/2025 02:42:00 - INFO -   \t>>>  R@1: 37.6 - R@5: 64.3 - R@10: 74.2 - Median R: 3.0 - Mean R: 22.4\n","04/30/2025 02:42:00 - INFO -   Video-to-Text:\n","04/30/2025 02:42:00 - INFO -   \t>>>  V2T$R@1: 35.6 - V2T$R@5: 62.5 - V2T$R@10: 73.3 - V2T$Median R: 3.0 - V2T$Mean R: 18.7\n","04/30/2025 02:42:00 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 10:  25% 70/281 [05:18<08:31,  2.42s/it, loss=0.131]04/30/2025 02:47:19 - INFO -   Epoch: 10/20, Step: 71/281, Lr: , Loss: 0.130565, Time/step: 3.187300\n","Epoch 10:  60% 170/281 [12:52<08:58,  4.85s/it, loss=0.114]04/30/2025 02:54:53 - INFO -   Epoch: 10/20, Step: 171/281, Lr: , Loss: 0.113492, Time/step: 4.539423\n","Epoch 10:  96% 270/281 [19:49<00:23,  2.15s/it, loss=0.26]  04/30/2025 03:01:49 - INFO -   Epoch: 10/20, Step: 271/281, Lr: , Loss: 0.260354, Time/step: 4.165573\n","Epoch 10: 100% 281/281 [20:19<00:00,  4.34s/it, loss=0.21]\n","04/30/2025 03:02:20 - INFO -   Epoch 10/20 Finished, Train Loss: 0.137356\n","04/30/2025 03:02:21 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.9\n","04/30/2025 03:02:21 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.9\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/30/2025 03:03:39 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 03:03:39 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 03:03:39 - INFO -   Text-to-Video:\n","04/30/2025 03:03:39 - INFO -   \t>>>  R@1: 37.2 - R@5: 62.0 - R@10: 74.3 - Median R: 3.0 - Mean R: 22.5\n","04/30/2025 03:03:39 - INFO -   Video-to-Text:\n","04/30/2025 03:03:39 - INFO -   \t>>>  V2T$R@1: 34.5 - V2T$R@5: 61.8 - V2T$R@10: 72.3 - V2T$Median R: 3.0 - V2T$Mean R: 18.5\n","04/30/2025 03:03:39 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 38.2000\n","Epoch 11:  32% 89/281 [06:54<25:46,  8.06s/it, loss=0.147]04/30/2025 03:10:34 - INFO -   Epoch: 11/20, Step: 90/281, Lr: , Loss: 0.147379, Time/step: 4.151042\n","Epoch 11:  67% 189/281 [14:00<07:58,  5.20s/it, loss=0.209] 04/30/2025 03:17:40 - INFO -   Epoch: 11/20, Step: 190/281, Lr: , Loss: 0.208956, Time/step: 4.258143\n","Epoch 11: 100% 281/281 [20:09<00:00,  4.30s/it, loss=0.137]\n","04/30/2025 03:23:49 - INFO -   Epoch 11/20 Finished, Train Loss: 0.128048\n","04/30/2025 03:23:50 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10\n","04/30/2025 03:23:50 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.10\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 03:25:08 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 03:25:08 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 03:25:08 - INFO -   Text-to-Video:\n","04/30/2025 03:25:08 - INFO -   \t>>>  R@1: 38.3 - R@5: 62.8 - R@10: 74.2 - Median R: 3.0 - Mean R: 22.3\n","04/30/2025 03:25:08 - INFO -   Video-to-Text:\n","04/30/2025 03:25:08 - INFO -   \t>>>  V2T$R@1: 34.8 - V2T$R@5: 61.1 - V2T$R@10: 72.6 - V2T$Median R: 3.0 - V2T$Mean R: 19.2\n","04/30/2025 03:25:08 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 12:   3% 8/281 [01:08<07:12,  1.59s/it, loss=0.16]  04/30/2025 03:26:16 - INFO -   Epoch: 12/20, Step: 9/281, Lr: , Loss: 0.159630, Time/step: 0.685847\n","Epoch 12:  38% 108/281 [08:06<11:40,  4.05s/it, loss=0.08] 04/30/2025 03:33:15 - INFO -   Epoch: 12/20, Step: 109/281, Lr: , Loss: 0.080035, Time/step: 4.182238\n","Epoch 12:  74% 208/281 [15:25<02:47,  2.30s/it, loss=0.0957]04/30/2025 03:40:34 - INFO -   Epoch: 12/20, Step: 209/281, Lr: , Loss: 0.095672, Time/step: 4.391632\n","Epoch 12: 100% 281/281 [20:13<00:00,  4.32s/it, loss=0.0973]\n","04/30/2025 03:45:22 - INFO -   Epoch 12/20 Finished, Train Loss: 0.122682\n","04/30/2025 03:45:23 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.11\n","04/30/2025 03:45:23 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.11\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 03:46:41 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 03:46:41 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 03:46:41 - INFO -   Text-to-Video:\n","04/30/2025 03:46:41 - INFO -   \t>>>  R@1: 38.1 - R@5: 63.2 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.5\n","04/30/2025 03:46:41 - INFO -   Video-to-Text:\n","04/30/2025 03:46:41 - INFO -   \t>>>  V2T$R@1: 34.8 - V2T$R@5: 60.9 - V2T$R@10: 72.8 - V2T$Median R: 3.0 - V2T$Mean R: 19.0\n","04/30/2025 03:46:41 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 13:  10% 27/281 [02:27<27:47,  6.57s/it, loss=0.095]04/30/2025 03:49:09 - INFO -   Epoch: 13/20, Step: 28/281, Lr: , Loss: 0.094969, Time/step: 1.480798\n","Epoch 13:  45% 127/281 [09:21<05:41,  2.22s/it, loss=0.133]04/30/2025 03:56:02 - INFO -   Epoch: 13/20, Step: 128/281, Lr: , Loss: 0.132682, Time/step: 4.136213\n","Epoch 13:  81% 227/281 [16:44<06:04,  6.74s/it, loss=0.112] 04/30/2025 04:03:25 - INFO -   Epoch: 13/20, Step: 228/281, Lr: , Loss: 0.112330, Time/step: 4.425965\n","Epoch 13: 100% 281/281 [20:08<00:00,  4.30s/it, loss=0.107]\n","04/30/2025 04:06:49 - INFO -   Epoch 13/20 Finished, Train Loss: 0.112066\n","04/30/2025 04:06:51 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.12\n","04/30/2025 04:06:51 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.12\n","Evaluating: 100% 125/125 [01:11<00:00,  1.76it/s]\n","04/30/2025 04:08:08 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:08:08 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:08:08 - INFO -   Text-to-Video:\n","04/30/2025 04:08:08 - INFO -   \t>>>  R@1: 37.3 - R@5: 63.0 - R@10: 74.1 - Median R: 3.0 - Mean R: 22.7\n","04/30/2025 04:08:08 - INFO -   Video-to-Text:\n","04/30/2025 04:08:08 - INFO -   \t>>>  V2T$R@1: 34.3 - V2T$R@5: 61.0 - V2T$R@10: 72.5 - V2T$Median R: 3.0 - V2T$Mean R: 19.1\n","04/30/2025 04:08:08 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 14:  16% 46/281 [03:41<09:12,  2.35s/it, loss=0.112] 04/30/2025 04:11:50 - INFO -   Epoch: 14/20, Step: 47/281, Lr: , Loss: 0.112007, Time/step: 2.219705\n","Epoch 14:  52% 146/281 [11:12<15:25,  6.86s/it, loss=0.114] 04/30/2025 04:19:21 - INFO -   Epoch: 14/20, Step: 147/281, Lr: , Loss: 0.113973, Time/step: 4.507420\n","Epoch 14:  88% 246/281 [17:57<01:08,  1.97s/it, loss=0.0844]04/30/2025 04:26:06 - INFO -   Epoch: 14/20, Step: 247/281, Lr: , Loss: 0.084442, Time/step: 4.052812\n","Epoch 14: 100% 281/281 [20:11<00:00,  4.31s/it, loss=0.0609]\n","04/30/2025 04:28:20 - INFO -   Epoch 14/20 Finished, Train Loss: 0.110388\n","04/30/2025 04:28:21 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.13\n","04/30/2025 04:28:21 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.13\n","Evaluating: 100% 125/125 [01:11<00:00,  1.74it/s]\n","04/30/2025 04:29:39 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:29:39 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:29:39 - INFO -   Text-to-Video:\n","04/30/2025 04:29:39 - INFO -   \t>>>  R@1: 37.2 - R@5: 63.8 - R@10: 73.8 - Median R: 3.0 - Mean R: 22.8\n","04/30/2025 04:29:39 - INFO -   Video-to-Text:\n","04/30/2025 04:29:39 - INFO -   \t>>>  V2T$R@1: 33.9 - V2T$R@5: 61.3 - V2T$R@10: 72.1 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 04:29:39 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 15:  23% 65/281 [05:09<33:21,  9.27s/it, loss=0.0535]04/30/2025 04:34:49 - INFO -   Epoch: 15/20, Step: 66/281, Lr: , Loss: 0.053548, Time/step: 3.098731\n","Epoch 15:  59% 165/281 [12:09<07:00,  3.63s/it, loss=0.131]04/30/2025 04:41:49 - INFO -   Epoch: 15/20, Step: 166/281, Lr: , Loss: 0.131140, Time/step: 4.196032\n","Epoch 15:  94% 265/281 [19:31<02:19,  8.73s/it, loss=0.157]04/30/2025 04:49:11 - INFO -   Epoch: 15/20, Step: 266/281, Lr: , Loss: 0.157224, Time/step: 4.425254\n","Epoch 15: 100% 281/281 [20:12<00:00,  4.32s/it, loss=0.0551]\n","04/30/2025 04:49:52 - INFO -   Epoch 15/20 Finished, Train Loss: 0.111329\n","04/30/2025 04:49:54 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.14\n","04/30/2025 04:49:54 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.14\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 04:51:11 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:51:11 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:51:11 - INFO -   Text-to-Video:\n","04/30/2025 04:51:11 - INFO -   \t>>>  R@1: 37.2 - R@5: 63.3 - R@10: 73.4 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 04:51:11 - INFO -   Video-to-Text:\n","04/30/2025 04:51:11 - INFO -   \t>>>  V2T$R@1: 34.1 - V2T$R@5: 60.8 - V2T$R@10: 72.3 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 04:51:11 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 16:  30% 84/281 [06:20<11:45,  3.58s/it, loss=0.156]04/30/2025 04:57:31 - INFO -   Epoch: 16/20, Step: 85/281, Lr: , Loss: 0.156052, Time/step: 3.803460\n","Epoch 16:  65% 184/281 [13:47<02:54,  1.80s/it, loss=0.137] 04/30/2025 05:04:59 - INFO -   Epoch: 16/20, Step: 185/281, Lr: , Loss: 0.137111, Time/step: 4.477574\n","Epoch 16: 100% 281/281 [20:12<00:00,  4.31s/it, loss=0.0129]\n","04/30/2025 05:11:24 - INFO -   Epoch 16/20 Finished, Train Loss: 0.102555\n","04/30/2025 05:11:25 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.15\n","04/30/2025 05:11:25 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.15\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/30/2025 05:12:44 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:12:44 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:12:44 - INFO -   Text-to-Video:\n","04/30/2025 05:12:44 - INFO -   \t>>>  R@1: 37.2 - R@5: 63.9 - R@10: 73.3 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 05:12:44 - INFO -   Video-to-Text:\n","04/30/2025 05:12:44 - INFO -   \t>>>  V2T$R@1: 33.8 - V2T$R@5: 61.2 - V2T$R@10: 72.5 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 05:12:44 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 17:   1% 3/281 [00:37<39:46,  8.58s/it, loss=0.115] 04/30/2025 05:13:21 - INFO -   Epoch: 17/20, Step: 4/281, Lr: , Loss: 0.114758, Time/step: 0.378470\n","Epoch 17:  37% 103/281 [07:38<08:06,  2.73s/it, loss=0.174] 04/30/2025 05:20:22 - INFO -   Epoch: 17/20, Step: 104/281, Lr: , Loss: 0.174198, Time/step: 4.207619\n","Epoch 17:  72% 203/281 [15:03<06:53,  5.30s/it, loss=0.103]04/30/2025 05:27:47 - INFO -   Epoch: 17/20, Step: 204/281, Lr: , Loss: 0.103544, Time/step: 4.447150\n","Epoch 17: 100% 281/281 [20:16<00:00,  4.33s/it, loss=0.0865]\n","04/30/2025 05:33:00 - INFO -   Epoch 17/20 Finished, Train Loss: 0.102144\n","04/30/2025 05:33:01 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.16\n","04/30/2025 05:33:01 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.16\n","Evaluating: 100% 125/125 [01:10<00:00,  1.76it/s]\n","04/30/2025 05:34:19 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:34:19 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:34:19 - INFO -   Text-to-Video:\n","04/30/2025 05:34:19 - INFO -   \t>>>  R@1: 37.2 - R@5: 63.6 - R@10: 73.3 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 05:34:19 - INFO -   Video-to-Text:\n","04/30/2025 05:34:19 - INFO -   \t>>>  V2T$R@1: 33.9 - V2T$R@5: 61.0 - V2T$R@10: 72.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 05:34:19 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 18:   8% 22/281 [01:51<09:31,  2.21s/it, loss=0.152]04/30/2025 05:36:11 - INFO -   Epoch: 18/20, Step: 23/281, Lr: , Loss: 0.152072, Time/step: 1.119607\n","Epoch 18:  43% 122/281 [09:18<18:25,  6.96s/it, loss=0.0592]04/30/2025 05:43:38 - INFO -   Epoch: 18/20, Step: 123/281, Lr: , Loss: 0.059163, Time/step: 4.468400\n","Epoch 18:  79% 222/281 [16:23<02:32,  2.58s/it, loss=0.0685]04/30/2025 05:50:42 - INFO -   Epoch: 18/20, Step: 223/281, Lr: , Loss: 0.068539, Time/step: 4.247126\n","Epoch 18: 100% 281/281 [20:27<00:00,  4.37s/it, loss=0.0311]\n","04/30/2025 05:54:47 - INFO -   Epoch 18/20 Finished, Train Loss: 0.100939\n","04/30/2025 05:54:48 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.17\n","04/30/2025 05:54:48 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.17\n","Evaluating: 100% 125/125 [01:13<00:00,  1.70it/s]\n","04/30/2025 05:56:08 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:56:08 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:56:08 - INFO -   Text-to-Video:\n","04/30/2025 05:56:08 - INFO -   \t>>>  R@1: 37.1 - R@5: 63.7 - R@10: 73.3 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 05:56:08 - INFO -   Video-to-Text:\n","04/30/2025 05:56:08 - INFO -   \t>>>  V2T$R@1: 34.0 - V2T$R@5: 60.9 - V2T$R@10: 72.3 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 05:56:08 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 19:  15% 41/281 [03:44<48:40, 12.17s/it, loss=0.174]04/30/2025 05:59:53 - INFO -   Epoch: 19/20, Step: 42/281, Lr: , Loss: 0.174354, Time/step: 2.248620\n","Epoch 19:  50% 141/281 [10:41<06:14,  2.67s/it, loss=0.13] 04/30/2025 06:06:49 - INFO -   Epoch: 19/20, Step: 142/281, Lr: , Loss: 0.130331, Time/step: 4.167520\n","Epoch 19:  86% 241/281 [18:15<06:49, 10.24s/it, loss=0.147]04/30/2025 06:14:24 - INFO -   Epoch: 19/20, Step: 242/281, Lr: , Loss: 0.147180, Time/step: 4.542367\n","Epoch 19: 100% 281/281 [20:37<00:00,  4.40s/it, loss=0.0512]\n","04/30/2025 06:16:45 - INFO -   Epoch 19/20 Finished, Train Loss: 0.107084\n","04/30/2025 06:16:47 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.18\n","04/30/2025 06:16:47 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.18\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/30/2025 06:18:05 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 06:18:05 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 06:18:05 - INFO -   Text-to-Video:\n","04/30/2025 06:18:05 - INFO -   \t>>>  R@1: 37.1 - R@5: 63.7 - R@10: 73.3 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 06:18:05 - INFO -   Video-to-Text:\n","04/30/2025 06:18:05 - INFO -   \t>>>  V2T$R@1: 34.0 - V2T$R@5: 60.9 - V2T$R@10: 72.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 06:18:06 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n","Epoch 20:  21% 60/281 [04:44<17:09,  4.66s/it, loss=0.0897]04/30/2025 06:22:50 - INFO -   Epoch: 20/20, Step: 61/281, Lr: , Loss: 0.089728, Time/step: 2.841974\n","Epoch 20:  57% 160/281 [12:13<03:38,  1.81s/it, loss=0.115]04/30/2025 06:30:19 - INFO -   Epoch: 20/20, Step: 161/281, Lr: , Loss: 0.114609, Time/step: 4.490054\n","Epoch 20:  93% 260/281 [19:23<01:27,  4.15s/it, loss=0.128]04/30/2025 06:37:29 - INFO -   Epoch: 20/20, Step: 261/281, Lr: , Loss: 0.128100, Time/step: 4.306258\n","Epoch 20: 100% 281/281 [20:34<00:00,  4.39s/it, loss=0.0872]\n","04/30/2025 06:38:40 - INFO -   Epoch 20/20 Finished, Train Loss: 0.101411\n","04/30/2025 06:38:42 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.19\n","04/30/2025 06:38:42 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.19\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/30/2025 06:40:01 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 06:40:01 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 06:40:01 - INFO -   Text-to-Video:\n","04/30/2025 06:40:01 - INFO -   \t>>>  R@1: 37.1 - R@5: 63.7 - R@10: 73.3 - Median R: 3.0 - Mean R: 23.0\n","04/30/2025 06:40:01 - INFO -   Video-to-Text:\n","04/30/2025 06:40:01 - INFO -   \t>>>  V2T$R@1: 34.0 - V2T$R@5: 60.9 - V2T$R@10: 72.3 - V2T$Median R: 3.0 - V2T$Mean R: 19.3\n","04/30/2025 06:40:01 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10, the R1 is: 38.3000\n"]}],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","!python /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/main_task_retrieval.py \\\n","  --do_train \\\n","  --num_thread_reader 8 \\\n","  --epochs 20 \\\n","  --batch_size 64 \\\n","  --train_csv  \"$DATA_PATH\"/MSRVTT_train.subset.csv \\\n","  --val_csv    \"$DATA_PATH\"/MSRVTT_JSFUSION_test.csv \\\n","  --data_path  \"$DATA_PATH\"/MSRVTT_data.json \\\n","  --features_path \"$VIDEO_PATH\" \\\n","  --output_dir /content/drive/MyDrive/CLIP_video_training/CLIP_video_5/ckpts/ckpt_msrvtt_retrieval_looseType \\\n","  --lr 1e-4 \\\n","  --max_words 32 \\\n","  --max_frames 8 \\\n","  --batch_size_val 8 \\\n","  --datatype msrvtt \\\n","  --expand_msrvtt_sentences \\\n","  --feature_framerate 1 \\\n","  --coef_lr 1e-3 \\\n","  --freeze_layer_num 6 \\\n","  --slice_framepos 5 \\\n","  --loose_type \\\n","  --linear_patch 2d \\\n","  --sim_header meanP \\\n","  --pretrained_clip_name ViT-B/32 \\\n","  --fp16 \\\n","  --fp16_opt_level O1\n"]},{"cell_type":"code","source":[],"metadata":{"id":"D6jYr2zgl-fN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}