{"cells":[{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5e6Tn4Nfbqm","executionInfo":{"status":"ok","timestamp":1745965382344,"user_tz":240,"elapsed":115,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"103afbbc-33c2-4f34-ac8c-c78b0b3cde88"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"x3amXy8zo9_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745965385306,"user_tz":240,"elapsed":212,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"cc775921-505f-4b6e-ad27-9feac2210ade"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr 29 22:23:05 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n","| N/A   64C    P0             30W /   72W |       3MiB /  23034MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# Connet to google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhLFIYgIfi65","executionInfo":{"status":"ok","timestamp":1745965389427,"user_tz":240,"elapsed":704,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"31a5e3e6-eaa5-473b-a711-e3d1b642f113"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!cd '/content/drive/MyDrive/CLIP_video_training'"],"metadata":{"id":"P0KkIxOvgDql","executionInfo":{"status":"ok","timestamp":1745965392581,"user_tz":240,"elapsed":99,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/sijiasiga/CLIP_video.git '/content/drive/MyDrive/CLIP_video_training/CLIP_video'\n","# !git clone https://github.com/sijiasiga/CLIP_video.git"],"metadata":{"id":"JkNb7yOMfdEs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745965397827,"user_tz":240,"elapsed":1505,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"3c1adfff-b44b-4d3b-8291-97f687ef9bfb"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/drive/MyDrive/CLIP_video_training/CLIP_video'...\n","remote: Enumerating objects: 230, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 230 (delta 29), reused 0 (delta 0), pack-reused 187 (from 1)\u001b[K\n","Receiving objects: 100% (230/230), 1.51 MiB | 28.58 MiB/s, done.\n","Resolving deltas: 100% (128/128), done.\n"]}]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGfdhKIXipJr","outputId":"1aebcfef-b4e2-4836-80be-f55e77a5fc20","executionInfo":{"status":"ok","timestamp":1745965405195,"user_tz":240,"elapsed":4617,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: conda: command not found\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: botocore<1.39.0,>=1.38.5 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.5)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.12.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["# From CLIP\n","!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n","!pip install ftfy regex tqdm\n","!pip install opencv-python boto3 requests pandas"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"WAT-NYLMm22_","executionInfo":{"status":"ok","timestamp":1745965405208,"user_tz":240,"elapsed":8,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[],"source":["# # Comment if the file exits\n","# !wget -O /content/drive/MyDrive/Data/msrvtt_data.zip https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n","# !unzip /content/drive/MyDrive/Data/msrvtt_data.zip -d /content/drive/MyDrive/Data/"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"uYqhvAKVpQD9","executionInfo":{"status":"ok","timestamp":1745965407935,"user_tz":240,"elapsed":11,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[],"source":["DATA_PATH = \"/content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data\"  # Directory of MSRVTT data\n","VIDEO_PATH = \"/content/drive/MyDrive/CLIP_video_training/Data/video\"    # Directory of MSRVTT raw video"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"3U2GCKmL14G6","colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"status":"ok","timestamp":1745965568087,"user_tz":240,"elapsed":1155,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"c20dec02-958f-4833-843a-bd6f70059b17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size:  9000\n","Column names ['video_id']\n"]},{"output_type":"display_data","data":{"text/plain":["  video_id\n","0   video0\n","1   video1\n","2   video2\n","3   video3\n","4   video4"],"text/html":["\n","  <div id=\"df-a3736ea9-5ba7-4b41-8d84-3da2eb700326\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>video0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>video1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3736ea9-5ba7-4b41-8d84-3da2eb700326')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a3736ea9-5ba7-4b41-8d84-3da2eb700326 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a3736ea9-5ba7-4b41-8d84-3da2eb700326');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-a195d4b2-c5a7-468b-b654-aa1705dfa831\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a195d4b2-c5a7-468b-b654-aa1705dfa831')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-a195d4b2-c5a7-468b-b654-aa1705dfa831 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"Subset CSV saved to:\\\", subset_path)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video1\",\n          \"video4\",\n          \"video2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Size of subset： 900\n"]},{"output_type":"display_data","data":{"text/plain":["    video_id\n","0  video8405\n","1  video1162\n","2   video582\n","3  video4081\n","4  video9139"],"text/html":["\n","  <div id=\"df-d24fc654-7c9f-42dc-bd4b-0b5b859f9298\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>video8405</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>video1162</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video582</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video4081</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video9139</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d24fc654-7c9f-42dc-bd4b-0b5b859f9298')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d24fc654-7c9f-42dc-bd4b-0b5b859f9298 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d24fc654-7c9f-42dc-bd4b-0b5b859f9298');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-597e046d-21ca-467e-b816-26b3f739ff1c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-597e046d-21ca-467e-b816-26b3f739ff1c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-597e046d-21ca-467e-b816-26b3f739ff1c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"Subset CSV saved to:\\\", subset_path)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"video1162\",\n          \"video9139\",\n          \"video582\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Subset CSV saved to: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_train.subset.csv\n"]}],"source":["import pandas as pd\n","\n","# Load training dataset\n","train_path = f\"{DATA_PATH}/MSRVTT_train.9k.csv\"\n","df = pd.read_csv(train_path)\n","\n","print(\"Size: \", len(df))\n","print(\"Column names\", df.columns.tolist())\n","display(df.head())\n","\n","# Choose a subset\n","def sample_subset(df, frac=0.1, random_state=42):\n","    return df.sample(frac=frac, random_state=random_state).reset_index(drop=True)\n","\n","# Set fraction to 0.01\n","subset_df = sample_subset(df, frac=0.1)\n","print(\"Size of subset：\", len(subset_df))\n","display(subset_df.head())\n","\n","# Save the subset\n","subset_path = f\"{DATA_PATH}/MSRVTT_train.subset.csv\"\n","subset_df.to_csv(subset_path, index=False)\n","\n","print(\"Subset CSV saved to:\", subset_path)\n"]},{"cell_type":"markdown","metadata":{"id":"4reTc9BCrqJx"},"source":[]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9E80wSjrI7z","outputId":"26cede15-93f1-4c75-df03-a229d8e5034e","executionInfo":{"status":"ok","timestamp":1745965414306,"user_tz":240,"elapsed":16,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Current device: NVIDIA L4\n"]}],"source":["import torch\n","print(\"Current device:\", torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU only\")"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"VyebJOr2s_8n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745965433315,"user_tz":240,"elapsed":6733,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"e6f8c2e9-2ced-4e53-9ca0-7b71f646676a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-29 22:23:46--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n","Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.59, 2620:1ec:bdf::59\n","Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.59|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 353976522 (338M) [application/octet-stream]\n","Saving to: ‘/content/drive/MyDrive/CLIP_video_training/CLIP_video/modules/ViT-B-32.pt’\n","\n","ViT-B-32.pt         100%[===================>] 337.58M  48.9MB/s    in 6.6s    \n","\n","2025-04-29 22:23:53 (51.3 MB/s) - ‘/content/drive/MyDrive/CLIP_video_training/CLIP_video/modules/ViT-B-32.pt’ saved [353976522/353976522]\n","\n"]}],"source":["!wget -P /content/drive/MyDrive/CLIP_video_training/CLIP_video/modules https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"6VDrwhRKsqgm","executionInfo":{"status":"ok","timestamp":1745965480294,"user_tz":240,"elapsed":40,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}}},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fp8YAC93w3z7"},"outputs":[],"source":["# # Updata changes to files. Comment if the files are not changed\n","# !cd CLIP_video && git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIV0xcbGPydW"},"outputs":[],"source":["# !pip install ffmpeg\n","# !python /content/CLIP_video/preprocess/compress_video.py --input_root \"/content/drive/MyDrive/IDL/IDL Project/Data/video\" --output_root \"/content/drive/MyDrive/IDL/IDL Project/Data/compressed_video\""]},{"cell_type":"code","execution_count":84,"metadata":{"id":"kb8sD1HGuYxE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745992174404,"user_tz":240,"elapsed":26143046,"user":{"displayName":"Sijia Ma","userId":"12608441977201664124"}},"outputId":"73ebae13-88c6-4eb4-c26e-31cfe316784b"},"outputs":[{"output_type":"stream","name":"stdout","text":["04/29/2025 22:33:55 - INFO -   Effective parameters:\n","04/29/2025 22:33:55 - INFO -     <<< batch_size: 64\n","04/29/2025 22:33:55 - INFO -     <<< batch_size_val: 8\n","04/29/2025 22:33:55 - INFO -     <<< cache_dir: \n","04/29/2025 22:33:55 - INFO -     <<< coef_lr: 0.001\n","04/29/2025 22:33:55 - INFO -     <<< cross_model: cross-base\n","04/29/2025 22:33:55 - INFO -     <<< cross_num_hidden_layers: 4\n","04/29/2025 22:33:55 - INFO -     <<< data_path: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_data.json\n","04/29/2025 22:33:55 - INFO -     <<< datatype: msrvtt\n","04/29/2025 22:33:55 - INFO -     <<< do_eval: False\n","04/29/2025 22:33:55 - INFO -     <<< do_lower_case: False\n","04/29/2025 22:33:55 - INFO -     <<< do_pretrain: False\n","04/29/2025 22:33:55 - INFO -     <<< do_train: True\n","04/29/2025 22:33:55 - INFO -     <<< epochs: 20\n","04/29/2025 22:33:55 - INFO -     <<< eval_frame_order: 0\n","04/29/2025 22:33:55 - INFO -     <<< expand_msrvtt_sentences: True\n","04/29/2025 22:33:55 - INFO -     <<< feature_framerate: 1\n","04/29/2025 22:33:55 - INFO -     <<< features_path: /content/drive/MyDrive/CLIP_video_training/Data/video\n","04/29/2025 22:33:55 - INFO -     <<< fp16: True\n","04/29/2025 22:33:55 - INFO -     <<< fp16_opt_level: O1\n","04/29/2025 22:33:55 - INFO -     <<< freeze_layer_num: 6\n","04/29/2025 22:33:55 - INFO -     <<< gradient_accumulation_steps: 1\n","04/29/2025 22:33:55 - INFO -     <<< hard_negative_rate: 0.5\n","04/29/2025 22:33:55 - INFO -     <<< init_model: None\n","04/29/2025 22:33:55 - INFO -     <<< linear_patch: 2d\n","04/29/2025 22:33:55 - INFO -     <<< loose_type: True\n","04/29/2025 22:33:55 - INFO -     <<< lr: 0.0001\n","04/29/2025 22:33:55 - INFO -     <<< lr_decay: 0.9\n","04/29/2025 22:33:55 - INFO -     <<< margin: 0.1\n","04/29/2025 22:33:55 - INFO -     <<< max_frames: 8\n","04/29/2025 22:33:55 - INFO -     <<< max_words: 32\n","04/29/2025 22:33:55 - INFO -     <<< n_display: 100\n","04/29/2025 22:33:55 - INFO -     <<< n_gpu: 1\n","04/29/2025 22:33:55 - INFO -     <<< n_pair: 1\n","04/29/2025 22:33:55 - INFO -     <<< negative_weighting: 1\n","04/29/2025 22:33:55 - INFO -     <<< num_thread_reader: 8\n","04/29/2025 22:33:55 - INFO -     <<< output_dir: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType\n","04/29/2025 22:33:55 - INFO -     <<< pretrained_clip_name: ViT-B/32\n","04/29/2025 22:33:55 - INFO -     <<< rank: 0\n","04/29/2025 22:33:55 - INFO -     <<< resume_model: None\n","04/29/2025 22:33:55 - INFO -     <<< sampled_use_mil: False\n","04/29/2025 22:33:55 - INFO -     <<< seed: 42\n","04/29/2025 22:33:55 - INFO -     <<< sim_header: meanP\n","04/29/2025 22:33:55 - INFO -     <<< slice_framepos: 2\n","04/29/2025 22:33:55 - INFO -     <<< task_type: retrieval\n","04/29/2025 22:33:55 - INFO -     <<< text_num_hidden_layers: 12\n","04/29/2025 22:33:55 - INFO -     <<< train_csv: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_train.subset.csv\n","04/29/2025 22:33:55 - INFO -     <<< train_frame_order: 0\n","04/29/2025 22:33:55 - INFO -     <<< use_mil: False\n","04/29/2025 22:33:55 - INFO -     <<< val_csv: /content/drive/MyDrive/CLIP_video_training/Data/msrvtt_data/MSRVTT_JSFUSION_test.csv\n","04/29/2025 22:33:55 - INFO -     <<< video_dim: 1024\n","04/29/2025 22:33:55 - INFO -     <<< visual_num_hidden_layers: 12\n","04/29/2025 22:33:55 - INFO -     <<< warmup_proportion: 0.1\n","04/29/2025 22:33:55 - INFO -     <<< world_size: 1\n","04/29/2025 22:33:55 - INFO -   device: cuda n_gpu: 1\n","04/29/2025 22:33:56 - INFO -   loading archive file /content/drive/MyDrive/CLIP_video_training/CLIP_video/modules/cross-base\n","04/29/2025 22:33:56 - INFO -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"max_position_embeddings\": 128,\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 512\n","}\n","\n","04/29/2025 22:33:56 - INFO -   Weight doesn't exsits. /content/drive/MyDrive/CLIP_video_training/CLIP_video/modules/cross-base/cross_pytorch_model.bin\n","04/29/2025 22:33:56 - WARNING -   Stage-One:True, Stage-Two:False\n","04/29/2025 22:33:56 - WARNING -   Test retrieval by loose type.\n","04/29/2025 22:33:56 - WARNING -   \t embed_dim: 512\n","04/29/2025 22:33:56 - WARNING -   \t image_resolution: 224\n","04/29/2025 22:33:56 - WARNING -   \t vision_layers: 12\n","04/29/2025 22:33:56 - WARNING -   \t vision_width: 768\n","04/29/2025 22:33:56 - WARNING -   \t vision_patch_size: 32\n","04/29/2025 22:33:56 - WARNING -   \t context_length: 77\n","04/29/2025 22:33:56 - WARNING -   \t vocab_size: 49408\n","04/29/2025 22:33:56 - WARNING -   \t transformer_width: 512\n","04/29/2025 22:33:56 - WARNING -   \t transformer_heads: 8\n","04/29/2025 22:33:56 - WARNING -   \t transformer_layers: 12\n","04/29/2025 22:33:56 - WARNING -   \t\t linear_patch: 2d\n","04/29/2025 22:33:56 - WARNING -   \t cut_top_layer: 0\n","04/29/2025 22:33:57 - WARNING -   \t sim_header: meanP\n","04/29/2025 22:34:02 - INFO -   --------------------\n","04/29/2025 22:34:02 - INFO -   Weights from pretrained model not used in CLIP4Clip: \n","   clip.input_resolution\n","   clip.context_length\n","   clip.vocab_size\n","04/29/2025 22:34:02 - INFO -   ***** Running test *****\n","04/29/2025 22:34:02 - INFO -     Num examples = 1000\n","04/29/2025 22:34:02 - INFO -     Batch size = 8\n","04/29/2025 22:34:02 - INFO -     Num steps = 125\n","04/29/2025 22:34:02 - INFO -   ***** Running val *****\n","04/29/2025 22:34:02 - INFO -     Num examples = 1000\n","04/29/2025 22:34:05 - INFO -   ***** Running training *****\n","04/29/2025 22:34:05 - INFO -     Num examples = 18000\n","04/29/2025 22:34:05 - INFO -     Batch size = 64\n","04/29/2025 22:34:05 - INFO -     Num steps = 5620\n","Epoch 1:  35% 99/281 [07:31<15:13,  5.02s/it, loss=1.09]04/29/2025 22:41:36 - INFO -   Epoch: 1/20, Step: 100/281, Lr: , Loss: 1.085586, Time/step: 4.516477\n","Epoch 1:  71% 199/281 [14:34<02:52,  2.10s/it, loss=0.814]04/29/2025 22:48:40 - INFO -   Epoch: 1/20, Step: 200/281, Lr: , Loss: 0.813534, Time/step: 4.234513\n","Epoch 1: 100% 281/281 [20:06<00:00,  4.29s/it, loss=0.746]\n","04/29/2025 22:54:12 - INFO -   Epoch 1/20 Finished, Train Loss: 1.167961\n","04/29/2025 22:54:13 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0\n","04/29/2025 22:54:13 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.0\n","Evaluating: 100% 125/125 [03:02<00:00,  1.46s/it]\n","04/29/2025 22:57:22 - INFO -   sim matrix size: 1000, 1000\n","04/29/2025 22:57:22 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/29/2025 22:57:22 - INFO -   Text-to-Video:\n","04/29/2025 22:57:22 - INFO -   \t>>>  R@1: 37.3 - R@5: 62.7 - R@10: 72.9 - Median R: 3.0 - Mean R: 21.0\n","04/29/2025 22:57:22 - INFO -   Video-to-Text:\n","04/29/2025 22:57:22 - INFO -   \t>>>  V2T$R@1: 38.3 - V2T$R@5: 64.1 - V2T$R@10: 74.4 - V2T$Median R: 3.0 - V2T$Mean R: 18.3\n","04/29/2025 22:57:22 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0, the R1 is: 37.3000\n","Epoch 2:   6% 18/281 [01:44<28:11,  6.43s/it, loss=0.911]04/29/2025 22:59:06 - INFO -   Epoch: 2/20, Step: 19/281, Lr: , Loss: 0.911053, Time/step: 1.042459\n","Epoch 2:  42% 118/281 [08:46<06:50,  2.52s/it, loss=0.839]04/29/2025 23:06:08 - INFO -   Epoch: 2/20, Step: 119/281, Lr: , Loss: 0.839295, Time/step: 4.222756\n","Epoch 2:  78% 218/281 [16:06<06:02,  5.75s/it, loss=0.522]04/29/2025 23:13:28 - INFO -   Epoch: 2/20, Step: 219/281, Lr: , Loss: 0.522485, Time/step: 4.397940\n","Epoch 2: 100% 281/281 [20:12<00:00,  4.31s/it, loss=0.708]\n","04/29/2025 23:17:35 - INFO -   Epoch 2/20 Finished, Train Loss: 0.713647\n","04/29/2025 23:17:36 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1\n","04/29/2025 23:17:36 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.1\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/29/2025 23:18:55 - INFO -   sim matrix size: 1000, 1000\n","04/29/2025 23:18:55 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/29/2025 23:18:55 - INFO -   Text-to-Video:\n","04/29/2025 23:18:55 - INFO -   \t>>>  R@1: 37.1 - R@5: 63.2 - R@10: 74.9 - Median R: 3.0 - Mean R: 20.0\n","04/29/2025 23:18:55 - INFO -   Video-to-Text:\n","04/29/2025 23:18:55 - INFO -   \t>>>  V2T$R@1: 37.5 - V2T$R@5: 65.8 - V2T$R@10: 77.1 - V2T$Median R: 3.0 - V2T$Mean R: 17.0\n","04/29/2025 23:18:55 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0, the R1 is: 37.3000\n","Epoch 3:  13% 37/281 [03:01<13:55,  3.43s/it, loss=0.341]04/29/2025 23:21:57 - INFO -   Epoch: 3/20, Step: 38/281, Lr: , Loss: 0.341388, Time/step: 1.821547\n","Epoch 3:  49% 137/281 [10:28<07:33,  3.15s/it, loss=0.439]04/29/2025 23:29:24 - INFO -   Epoch: 3/20, Step: 138/281, Lr: , Loss: 0.438616, Time/step: 4.465675\n","Epoch 3:  84% 237/281 [17:27<02:54,  3.98s/it, loss=0.34] 04/29/2025 23:36:22 - INFO -   Epoch: 3/20, Step: 238/281, Lr: , Loss: 0.340434, Time/step: 4.185112\n","Epoch 3: 100% 281/281 [20:15<00:00,  4.33s/it, loss=0.439]\n","04/29/2025 23:39:11 - INFO -   Epoch 3/20 Finished, Train Loss: 0.488119\n","04/29/2025 23:39:13 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2\n","04/29/2025 23:39:13 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.2\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/29/2025 23:40:31 - INFO -   sim matrix size: 1000, 1000\n","04/29/2025 23:40:31 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/29/2025 23:40:31 - INFO -   Text-to-Video:\n","04/29/2025 23:40:31 - INFO -   \t>>>  R@1: 37.7 - R@5: 63.9 - R@10: 74.9 - Median R: 3.0 - Mean R: 20.8\n","04/29/2025 23:40:31 - INFO -   Video-to-Text:\n","04/29/2025 23:40:31 - INFO -   \t>>>  V2T$R@1: 38.2 - V2T$R@5: 63.8 - V2T$R@10: 76.2 - V2T$Median R: 3.0 - V2T$Mean R: 16.2\n","04/29/2025 23:40:31 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2, the R1 is: 37.7000\n","Epoch 4:  20% 56/281 [04:44<07:01,  1.87s/it, loss=0.262]04/29/2025 23:45:16 - INFO -   Epoch: 4/20, Step: 57/281, Lr: , Loss: 0.262313, Time/step: 2.851841\n","Epoch 4:  56% 156/281 [11:42<07:49,  3.76s/it, loss=0.351]04/29/2025 23:52:14 - INFO -   Epoch: 4/20, Step: 157/281, Lr: , Loss: 0.351077, Time/step: 4.179645\n","Epoch 4:  91% 256/281 [19:01<00:57,  2.31s/it, loss=0.259]04/29/2025 23:59:33 - INFO -   Epoch: 4/20, Step: 257/281, Lr: , Loss: 0.259181, Time/step: 4.388250\n","Epoch 4: 100% 281/281 [20:14<00:00,  4.32s/it, loss=0.386]\n","04/30/2025 00:00:46 - INFO -   Epoch 4/20 Finished, Train Loss: 0.354629\n","04/30/2025 00:00:47 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3\n","04/30/2025 00:00:47 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.3\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/30/2025 00:02:05 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:02:05 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:02:05 - INFO -   Text-to-Video:\n","04/30/2025 00:02:05 - INFO -   \t>>>  R@1: 37.6 - R@5: 63.3 - R@10: 74.6 - Median R: 3.0 - Mean R: 22.1\n","04/30/2025 00:02:05 - INFO -   Video-to-Text:\n","04/30/2025 00:02:05 - INFO -   \t>>>  V2T$R@1: 36.3 - V2T$R@5: 62.8 - V2T$R@10: 74.3 - V2T$Median R: 3.0 - V2T$Mean R: 18.2\n","04/30/2025 00:02:05 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2, the R1 is: 37.7000\n","Epoch 5:  27% 75/281 [05:55<21:29,  6.26s/it, loss=0.268]04/30/2025 00:08:01 - INFO -   Epoch: 5/20, Step: 76/281, Lr: , Loss: 0.268322, Time/step: 3.556916\n","Epoch 5:  62% 175/281 [13:05<05:08,  2.91s/it, loss=0.252]04/30/2025 00:15:11 - INFO -   Epoch: 5/20, Step: 176/281, Lr: , Loss: 0.252220, Time/step: 4.296062\n","Epoch 5:  98% 275/281 [20:12<00:36,  6.04s/it, loss=0.221]04/30/2025 00:22:18 - INFO -   Epoch: 5/20, Step: 276/281, Lr: , Loss: 0.220700, Time/step: 4.274301\n","Epoch 5: 100% 281/281 [20:16<00:00,  4.33s/it, loss=0.217]\n","04/30/2025 00:22:23 - INFO -   Epoch 5/20 Finished, Train Loss: 0.268512\n","04/30/2025 00:22:24 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4\n","04/30/2025 00:22:24 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.4\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 00:23:42 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:23:42 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:23:42 - INFO -   Text-to-Video:\n","04/30/2025 00:23:42 - INFO -   \t>>>  R@1: 39.2 - R@5: 62.9 - R@10: 74.3 - Median R: 3.0 - Mean R: 22.0\n","04/30/2025 00:23:42 - INFO -   Video-to-Text:\n","04/30/2025 00:23:42 - INFO -   \t>>>  V2T$R@1: 36.7 - V2T$R@5: 63.1 - V2T$R@10: 74.1 - V2T$Median R: 3.0 - V2T$Mean R: 18.3\n","04/30/2025 00:23:42 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4, the R1 is: 39.2000\n","Epoch 6:  33% 94/281 [07:09<07:32,  2.42s/it, loss=0.19] 04/30/2025 00:30:52 - INFO -   Epoch: 6/20, Step: 95/281, Lr: , Loss: 0.190269, Time/step: 4.296893\n","Epoch 6:  69% 194/281 [14:39<10:05,  6.96s/it, loss=0.241]04/30/2025 00:38:22 - INFO -   Epoch: 6/20, Step: 195/281, Lr: , Loss: 0.240636, Time/step: 4.500401\n","Epoch 6: 100% 281/281 [20:15<00:00,  4.33s/it, loss=0.143]\n","04/30/2025 00:43:58 - INFO -   Epoch 6/20 Finished, Train Loss: 0.219370\n","04/30/2025 00:44:00 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.5\n","04/30/2025 00:44:00 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.5\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 00:45:17 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 00:45:17 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 00:45:17 - INFO -   Text-to-Video:\n","04/30/2025 00:45:17 - INFO -   \t>>>  R@1: 38.6 - R@5: 62.4 - R@10: 73.4 - Median R: 3.0 - Mean R: 21.9\n","04/30/2025 00:45:17 - INFO -   Video-to-Text:\n","04/30/2025 00:45:17 - INFO -   \t>>>  V2T$R@1: 35.9 - V2T$R@5: 63.4 - V2T$R@10: 73.6 - V2T$Median R: 3.0 - V2T$Mean R: 18.6\n","04/30/2025 00:45:17 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4, the R1 is: 39.2000\n","Epoch 7:   5% 13/281 [01:19<18:05,  4.05s/it, loss=0.0794]04/30/2025 00:46:37 - INFO -   Epoch: 7/20, Step: 14/281, Lr: , Loss: 0.079378, Time/step: 0.795364\n","Epoch 7:  40% 113/281 [08:33<20:14,  7.23s/it, loss=0.13] 04/30/2025 00:53:51 - INFO -   Epoch: 7/20, Step: 114/281, Lr: , Loss: 0.130060, Time/step: 4.339756\n","Epoch 7:  76% 213/281 [15:41<03:44,  3.30s/it, loss=0.237]04/30/2025 01:00:59 - INFO -   Epoch: 7/20, Step: 214/281, Lr: , Loss: 0.236558, Time/step: 4.279036\n","Epoch 7: 100% 281/281 [20:14<00:00,  4.32s/it, loss=0.187]\n","04/30/2025 01:05:32 - INFO -   Epoch 7/20 Finished, Train Loss: 0.190097\n","04/30/2025 01:05:34 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.6\n","04/30/2025 01:05:34 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.6\n","Evaluating: 100% 125/125 [01:10<00:00,  1.77it/s]\n","04/30/2025 01:06:51 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:06:51 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:06:51 - INFO -   Text-to-Video:\n","04/30/2025 01:06:51 - INFO -   \t>>>  R@1: 37.2 - R@5: 62.8 - R@10: 73.4 - Median R: 3.0 - Mean R: 22.8\n","04/30/2025 01:06:51 - INFO -   Video-to-Text:\n","04/30/2025 01:06:51 - INFO -   \t>>>  V2T$R@1: 36.4 - V2T$R@5: 62.8 - V2T$R@10: 72.4 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 01:06:51 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4, the R1 is: 39.2000\n","Epoch 8:  11% 32/281 [02:53<07:21,  1.77s/it, loss=0.136]04/30/2025 01:09:45 - INFO -   Epoch: 8/20, Step: 33/281, Lr: , Loss: 0.135801, Time/step: 1.741348\n","Epoch 8:  47% 132/281 [10:10<18:42,  7.54s/it, loss=0.113]04/30/2025 01:17:01 - INFO -   Epoch: 8/20, Step: 133/281, Lr: , Loss: 0.112510, Time/step: 4.361190\n","Epoch 8:  83% 232/281 [17:25<02:31,  3.10s/it, loss=0.102]04/30/2025 01:24:16 - INFO -   Epoch: 8/20, Step: 233/281, Lr: , Loss: 0.101685, Time/step: 4.353490\n","Epoch 8: 100% 281/281 [20:19<00:00,  4.34s/it, loss=0.164]\n","04/30/2025 01:27:11 - INFO -   Epoch 8/20 Finished, Train Loss: 0.163159\n","04/30/2025 01:27:12 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.7\n","04/30/2025 01:27:12 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.7\n","Evaluating: 100% 125/125 [01:10<00:00,  1.77it/s]\n","04/30/2025 01:28:29 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:28:29 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:28:29 - INFO -   Text-to-Video:\n","04/30/2025 01:28:29 - INFO -   \t>>>  R@1: 37.9 - R@5: 62.9 - R@10: 74.4 - Median R: 3.0 - Mean R: 22.3\n","04/30/2025 01:28:29 - INFO -   Video-to-Text:\n","04/30/2025 01:28:29 - INFO -   \t>>>  V2T$R@1: 33.9 - V2T$R@5: 62.5 - V2T$R@10: 72.8 - V2T$Median R: 3.0 - V2T$Mean R: 18.4\n","04/30/2025 01:28:29 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4, the R1 is: 39.2000\n","Epoch 9:  18% 51/281 [04:10<23:48,  6.21s/it, loss=0.143]04/30/2025 01:32:40 - INFO -   Epoch: 9/20, Step: 52/281, Lr: , Loss: 0.142582, Time/step: 2.507450\n","Epoch 9:  54% 151/281 [11:13<08:13,  3.79s/it, loss=0.108]04/30/2025 01:39:43 - INFO -   Epoch: 9/20, Step: 152/281, Lr: , Loss: 0.108342, Time/step: 4.232355\n","Epoch 9:  89% 251/281 [18:29<02:36,  5.22s/it, loss=0.0452]04/30/2025 01:46:59 - INFO -   Epoch: 9/20, Step: 252/281, Lr: , Loss: 0.045232, Time/step: 4.360349\n","Epoch 9: 100% 281/281 [20:12<00:00,  4.31s/it, loss=0.255]\n","04/30/2025 01:48:42 - INFO -   Epoch 9/20 Finished, Train Loss: 0.152565\n","04/30/2025 01:48:43 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8\n","04/30/2025 01:48:43 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.8\n","Evaluating: 100% 125/125 [01:10<00:00,  1.77it/s]\n","04/30/2025 01:50:00 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 01:50:00 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 01:50:00 - INFO -   Text-to-Video:\n","04/30/2025 01:50:00 - INFO -   \t>>>  R@1: 39.5 - R@5: 63.6 - R@10: 74.6 - Median R: 3.0 - Mean R: 22.3\n","04/30/2025 01:50:00 - INFO -   Video-to-Text:\n","04/30/2025 01:50:00 - INFO -   \t>>>  V2T$R@1: 35.9 - V2T$R@5: 63.3 - V2T$R@10: 73.9 - V2T$Median R: 3.0 - V2T$Mean R: 18.3\n","04/30/2025 01:50:00 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 10:  25% 70/281 [05:23<09:46,  2.78s/it, loss=0.123]04/30/2025 01:55:24 - INFO -   Epoch: 10/20, Step: 71/281, Lr: , Loss: 0.122553, Time/step: 3.237254\n","Epoch 10:  60% 170/281 [12:47<09:10,  4.96s/it, loss=0.116]04/30/2025 02:02:48 - INFO -   Epoch: 10/20, Step: 171/281, Lr: , Loss: 0.116162, Time/step: 4.438002\n","Epoch 10:  96% 270/281 [19:48<00:26,  2.44s/it, loss=0.246] 04/30/2025 02:09:50 - INFO -   Epoch: 10/20, Step: 271/281, Lr: , Loss: 0.246466, Time/step: 4.215305\n","Epoch 10: 100% 281/281 [20:15<00:00,  4.33s/it, loss=0.222]\n","04/30/2025 02:10:16 - INFO -   Epoch 10/20 Finished, Train Loss: 0.137065\n","04/30/2025 02:10:18 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.9\n","04/30/2025 02:10:18 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.9\n","Evaluating: 100% 125/125 [01:10<00:00,  1.77it/s]\n","04/30/2025 02:11:35 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 02:11:35 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 02:11:35 - INFO -   Text-to-Video:\n","04/30/2025 02:11:35 - INFO -   \t>>>  R@1: 38.7 - R@5: 62.1 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.1\n","04/30/2025 02:11:35 - INFO -   Video-to-Text:\n","04/30/2025 02:11:35 - INFO -   \t>>>  V2T$R@1: 34.5 - V2T$R@5: 61.8 - V2T$R@10: 72.4 - V2T$Median R: 3.0 - V2T$Mean R: 18.1\n","04/30/2025 02:11:35 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 11:  32% 89/281 [06:58<24:13,  7.57s/it, loss=0.146]04/30/2025 02:18:34 - INFO -   Epoch: 11/20, Step: 90/281, Lr: , Loss: 0.145965, Time/step: 4.189589\n","Epoch 11:  67% 189/281 [14:05<07:38,  4.98s/it, loss=0.203] 04/30/2025 02:25:40 - INFO -   Epoch: 11/20, Step: 190/281, Lr: , Loss: 0.203043, Time/step: 4.266055\n","Epoch 11: 100% 281/281 [20:13<00:00,  4.32s/it, loss=0.135]\n","04/30/2025 02:31:49 - INFO -   Epoch 11/20 Finished, Train Loss: 0.127628\n","04/30/2025 02:31:50 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.10\n","04/30/2025 02:31:50 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.10\n","Evaluating: 100% 125/125 [01:11<00:00,  1.74it/s]\n","04/30/2025 02:33:08 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 02:33:08 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 02:33:08 - INFO -   Text-to-Video:\n","04/30/2025 02:33:08 - INFO -   \t>>>  R@1: 38.9 - R@5: 63.2 - R@10: 74.3 - Median R: 3.0 - Mean R: 21.7\n","04/30/2025 02:33:08 - INFO -   Video-to-Text:\n","04/30/2025 02:33:08 - INFO -   \t>>>  V2T$R@1: 34.9 - V2T$R@5: 61.2 - V2T$R@10: 72.4 - V2T$Median R: 3.0 - V2T$Mean R: 18.5\n","04/30/2025 02:33:09 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 12:   3% 8/281 [01:09<08:36,  1.89s/it, loss=0.167] 04/30/2025 02:34:18 - INFO -   Epoch: 12/20, Step: 9/281, Lr: , Loss: 0.167165, Time/step: 0.698136\n","Epoch 12:  38% 108/281 [08:07<11:44,  4.07s/it, loss=0.074]04/30/2025 02:41:17 - INFO -   Epoch: 12/20, Step: 109/281, Lr: , Loss: 0.073968, Time/step: 4.183393\n","Epoch 12:  74% 208/281 [15:29<03:05,  2.54s/it, loss=0.103]04/30/2025 02:48:38 - INFO -   Epoch: 12/20, Step: 209/281, Lr: , Loss: 0.103507, Time/step: 4.414946\n","Epoch 12: 100% 281/281 [20:16<00:00,  4.33s/it, loss=0.0929]\n","04/30/2025 02:53:25 - INFO -   Epoch 12/20 Finished, Train Loss: 0.122836\n","04/30/2025 02:53:27 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.11\n","04/30/2025 02:53:27 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.11\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 02:54:44 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 02:54:44 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 02:54:44 - INFO -   Text-to-Video:\n","04/30/2025 02:54:44 - INFO -   \t>>>  R@1: 38.4 - R@5: 62.6 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.2\n","04/30/2025 02:54:44 - INFO -   Video-to-Text:\n","04/30/2025 02:54:44 - INFO -   \t>>>  V2T$R@1: 34.3 - V2T$R@5: 61.5 - V2T$R@10: 72.0 - V2T$Median R: 3.0 - V2T$Mean R: 18.6\n","04/30/2025 02:54:44 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 13:  10% 27/281 [02:27<25:41,  6.07s/it, loss=0.101]04/30/2025 02:57:12 - INFO -   Epoch: 13/20, Step: 28/281, Lr: , Loss: 0.101264, Time/step: 1.476136\n","Epoch 13:  45% 127/281 [09:25<06:32,  2.55s/it, loss=0.129]04/30/2025 03:04:11 - INFO -   Epoch: 13/20, Step: 128/281, Lr: , Loss: 0.128702, Time/step: 4.185478\n","Epoch 13:  81% 227/281 [16:49<06:04,  6.75s/it, loss=0.115] 04/30/2025 03:11:35 - INFO -   Epoch: 13/20, Step: 228/281, Lr: , Loss: 0.114813, Time/step: 4.439552\n","Epoch 13: 100% 281/281 [20:18<00:00,  4.34s/it, loss=0.104]\n","04/30/2025 03:15:04 - INFO -   Epoch 13/20 Finished, Train Loss: 0.112445\n","04/30/2025 03:15:05 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.12\n","04/30/2025 03:15:05 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.12\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/30/2025 03:16:24 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 03:16:24 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 03:16:24 - INFO -   Text-to-Video:\n","04/30/2025 03:16:24 - INFO -   \t>>>  R@1: 38.5 - R@5: 61.8 - R@10: 73.2 - Median R: 3.0 - Mean R: 22.3\n","04/30/2025 03:16:24 - INFO -   Video-to-Text:\n","04/30/2025 03:16:24 - INFO -   \t>>>  V2T$R@1: 34.7 - V2T$R@5: 62.2 - V2T$R@10: 72.7 - V2T$Median R: 3.0 - V2T$Mean R: 18.6\n","04/30/2025 03:16:24 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 14:  16% 46/281 [03:48<10:07,  2.59s/it, loss=0.112] 04/30/2025 03:20:13 - INFO -   Epoch: 14/20, Step: 47/281, Lr: , Loss: 0.112067, Time/step: 2.282711\n","Epoch 14:  52% 146/281 [11:22<14:43,  6.54s/it, loss=0.104] 04/30/2025 03:27:47 - INFO -   Epoch: 14/20, Step: 147/281, Lr: , Loss: 0.103592, Time/step: 4.542149\n","Epoch 14:  88% 246/281 [18:16<01:19,  2.26s/it, loss=0.088]04/30/2025 03:34:41 - INFO -   Epoch: 14/20, Step: 247/281, Lr: , Loss: 0.087972, Time/step: 4.146153\n","Epoch 14: 100% 281/281 [20:29<00:00,  4.38s/it, loss=0.0606]\n","04/30/2025 03:36:54 - INFO -   Epoch 14/20 Finished, Train Loss: 0.110528\n","04/30/2025 03:36:56 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.13\n","04/30/2025 03:36:56 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.13\n","Evaluating: 100% 125/125 [01:13<00:00,  1.71it/s]\n","04/30/2025 03:38:15 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 03:38:15 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 03:38:15 - INFO -   Text-to-Video:\n","04/30/2025 03:38:15 - INFO -   \t>>>  R@1: 38.0 - R@5: 62.3 - R@10: 73.5 - Median R: 3.0 - Mean R: 22.4\n","04/30/2025 03:38:15 - INFO -   Video-to-Text:\n","04/30/2025 03:38:15 - INFO -   \t>>>  V2T$R@1: 34.5 - V2T$R@5: 62.2 - V2T$R@10: 72.2 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 03:38:15 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 15:  23% 65/281 [05:15<34:32,  9.59s/it, loss=0.0566]04/30/2025 03:43:31 - INFO -   Epoch: 15/20, Step: 66/281, Lr: , Loss: 0.056552, Time/step: 3.160179\n","Epoch 15:  59% 165/281 [12:19<05:59,  3.10s/it, loss=0.137]04/30/2025 03:50:35 - INFO -   Epoch: 15/20, Step: 166/281, Lr: , Loss: 0.136635, Time/step: 4.232392\n","Epoch 15:  94% 265/281 [19:50<02:27,  9.21s/it, loss=0.158]04/30/2025 03:58:06 - INFO -   Epoch: 15/20, Step: 266/281, Lr: , Loss: 0.157588, Time/step: 4.510837\n","Epoch 15: 100% 281/281 [20:30<00:00,  4.38s/it, loss=0.0601]\n","04/30/2025 03:58:46 - INFO -   Epoch 15/20 Finished, Train Loss: 0.111437\n","04/30/2025 03:58:48 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.14\n","04/30/2025 03:58:48 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.14\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/30/2025 04:00:07 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:00:07 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:00:07 - INFO -   Text-to-Video:\n","04/30/2025 04:00:07 - INFO -   \t>>>  R@1: 38.2 - R@5: 62.5 - R@10: 73.1 - Median R: 3.0 - Mean R: 22.5\n","04/30/2025 04:00:07 - INFO -   Video-to-Text:\n","04/30/2025 04:00:07 - INFO -   \t>>>  V2T$R@1: 34.4 - V2T$R@5: 61.9 - V2T$R@10: 72.2 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 04:00:07 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 16:  30% 84/281 [06:26<12:01,  3.66s/it, loss=0.161] 04/30/2025 04:06:34 - INFO -   Epoch: 16/20, Step: 85/281, Lr: , Loss: 0.160502, Time/step: 3.867593\n","Epoch 16:  65% 184/281 [14:01<03:03,  1.89s/it, loss=0.136] 04/30/2025 04:14:09 - INFO -   Epoch: 16/20, Step: 185/281, Lr: , Loss: 0.135543, Time/step: 4.547635\n","Epoch 16: 100% 281/281 [20:31<00:00,  4.38s/it, loss=0.0112]\n","04/30/2025 04:20:39 - INFO -   Epoch 16/20 Finished, Train Loss: 0.102979\n","04/30/2025 04:20:40 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.15\n","04/30/2025 04:20:40 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.15\n","Evaluating: 100% 125/125 [01:11<00:00,  1.75it/s]\n","04/30/2025 04:21:58 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:21:58 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:21:58 - INFO -   Text-to-Video:\n","04/30/2025 04:21:58 - INFO -   \t>>>  R@1: 38.3 - R@5: 62.6 - R@10: 73.4 - Median R: 3.0 - Mean R: 22.6\n","04/30/2025 04:21:58 - INFO -   Video-to-Text:\n","04/30/2025 04:21:58 - INFO -   \t>>>  V2T$R@1: 34.5 - V2T$R@5: 62.0 - V2T$R@10: 72.6 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 04:21:58 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 17:   1% 3/281 [00:39<41:37,  8.98s/it, loss=0.121] 04/30/2025 04:22:38 - INFO -   Epoch: 17/20, Step: 4/281, Lr: , Loss: 0.120800, Time/step: 0.396124\n","Epoch 17:  37% 103/281 [07:46<09:12,  3.10s/it, loss=0.162] 04/30/2025 04:29:45 - INFO -   Epoch: 17/20, Step: 104/281, Lr: , Loss: 0.162084, Time/step: 4.272298\n","Epoch 17:  72% 203/281 [15:14<06:52,  5.29s/it, loss=0.108]04/30/2025 04:37:13 - INFO -   Epoch: 17/20, Step: 204/281, Lr: , Loss: 0.107605, Time/step: 4.482349\n","Epoch 17: 100% 281/281 [20:35<00:00,  4.40s/it, loss=0.0859]\n","04/30/2025 04:42:34 - INFO -   Epoch 17/20 Finished, Train Loss: 0.101500\n","04/30/2025 04:42:35 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.16\n","04/30/2025 04:42:35 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.16\n","Evaluating: 100% 125/125 [01:11<00:00,  1.74it/s]\n","04/30/2025 04:43:53 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 04:43:53 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 04:43:53 - INFO -   Text-to-Video:\n","04/30/2025 04:43:53 - INFO -   \t>>>  R@1: 38.3 - R@5: 62.3 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.6\n","04/30/2025 04:43:53 - INFO -   Video-to-Text:\n","04/30/2025 04:43:53 - INFO -   \t>>>  V2T$R@1: 34.5 - V2T$R@5: 61.8 - V2T$R@10: 72.5 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 04:43:53 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 18:   8% 22/281 [01:55<10:24,  2.41s/it, loss=0.155]04/30/2025 04:45:48 - INFO -   Epoch: 18/20, Step: 23/281, Lr: , Loss: 0.154980, Time/step: 1.152491\n","Epoch 18:  43% 122/281 [09:28<16:33,  6.25s/it, loss=0.0592]04/30/2025 04:53:22 - INFO -   Epoch: 18/20, Step: 123/281, Lr: , Loss: 0.059166, Time/step: 4.537342\n","Epoch 18:  79% 222/281 [16:32<02:44,  2.80s/it, loss=0.0711]04/30/2025 05:00:26 - INFO -   Epoch: 18/20, Step: 223/281, Lr: , Loss: 0.071070, Time/step: 4.235262\n","Epoch 18: 100% 281/281 [20:33<00:00,  4.39s/it, loss=0.0313]\n","04/30/2025 05:04:27 - INFO -   Epoch 18/20 Finished, Train Loss: 0.100870\n","04/30/2025 05:04:28 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.17\n","04/30/2025 05:04:28 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.17\n","Evaluating: 100% 125/125 [01:12<00:00,  1.72it/s]\n","04/30/2025 05:05:47 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:05:47 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:05:47 - INFO -   Text-to-Video:\n","04/30/2025 05:05:47 - INFO -   \t>>>  R@1: 38.4 - R@5: 62.3 - R@10: 73.2 - Median R: 3.0 - Mean R: 22.6\n","04/30/2025 05:05:47 - INFO -   Video-to-Text:\n","04/30/2025 05:05:47 - INFO -   \t>>>  V2T$R@1: 34.6 - V2T$R@5: 61.9 - V2T$R@10: 72.6 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 05:05:47 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 19:  15% 41/281 [03:43<46:39, 11.66s/it, loss=0.194]04/30/2025 05:09:31 - INFO -   Epoch: 19/20, Step: 42/281, Lr: , Loss: 0.193694, Time/step: 2.233121\n","Epoch 19:  50% 141/281 [10:39<06:37,  2.84s/it, loss=0.13] 04/30/2025 05:16:28 - INFO -   Epoch: 19/20, Step: 142/281, Lr: , Loss: 0.130186, Time/step: 4.168606\n","Epoch 19:  86% 241/281 [18:11<06:30,  9.76s/it, loss=0.152]04/30/2025 05:23:59 - INFO -   Epoch: 19/20, Step: 242/281, Lr: , Loss: 0.151883, Time/step: 4.512362\n","Epoch 19: 100% 281/281 [20:33<00:00,  4.39s/it, loss=0.0499]\n","04/30/2025 05:26:21 - INFO -   Epoch 19/20 Finished, Train Loss: 0.107701\n","04/30/2025 05:26:22 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.18\n","04/30/2025 05:26:22 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.18\n","Evaluating: 100% 125/125 [01:12<00:00,  1.73it/s]\n","04/30/2025 05:27:41 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:27:41 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:27:41 - INFO -   Text-to-Video:\n","04/30/2025 05:27:41 - INFO -   \t>>>  R@1: 38.4 - R@5: 62.3 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.6\n","04/30/2025 05:27:41 - INFO -   Video-to-Text:\n","04/30/2025 05:27:41 - INFO -   \t>>>  V2T$R@1: 34.6 - V2T$R@5: 62.0 - V2T$R@10: 72.5 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 05:27:41 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n","Epoch 20:  21% 60/281 [04:43<13:20,  3.62s/it, loss=0.0849]04/30/2025 05:32:25 - INFO -   Epoch: 20/20, Step: 61/281, Lr: , Loss: 0.084917, Time/step: 2.834198\n","Epoch 20:  57% 160/281 [12:14<03:57,  1.96s/it, loss=0.125] 04/30/2025 05:39:56 - INFO -   Epoch: 20/20, Step: 161/281, Lr: , Loss: 0.124974, Time/step: 4.511145\n","Epoch 20:  93% 260/281 [19:24<01:36,  4.58s/it, loss=0.137]04/30/2025 05:47:06 - INFO -   Epoch: 20/20, Step: 261/281, Lr: , Loss: 0.137230, Time/step: 4.302381\n","Epoch 20: 100% 281/281 [20:31<00:00,  4.38s/it, loss=0.0839]\n","04/30/2025 05:48:13 - INFO -   Epoch 20/20 Finished, Train Loss: 0.101418\n","04/30/2025 05:48:14 - INFO -   Model saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.19\n","04/30/2025 05:48:14 - INFO -   Optimizer saved to /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.19\n","Evaluating: 100% 125/125 [01:11<00:00,  1.74it/s]\n","04/30/2025 05:49:33 - INFO -   sim matrix size: 1000, 1000\n","04/30/2025 05:49:33 - INFO -   \t Length-T: 1000, Length-V:1000\n","04/30/2025 05:49:33 - INFO -   Text-to-Video:\n","04/30/2025 05:49:33 - INFO -   \t>>>  R@1: 38.3 - R@5: 62.3 - R@10: 73.3 - Median R: 3.0 - Mean R: 22.6\n","04/30/2025 05:49:33 - INFO -   Video-to-Text:\n","04/30/2025 05:49:33 - INFO -   \t>>>  V2T$R@1: 34.7 - V2T$R@5: 61.9 - V2T$R@10: 72.5 - V2T$Median R: 3.0 - V2T$Mean R: 18.8\n","04/30/2025 05:49:33 - INFO -   The best model is: /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.8, the R1 is: 39.5000\n"]}],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","!python /content/drive/MyDrive/CLIP_video_training/CLIP_video/main_task_retrieval.py \\\n","  --do_train \\\n","  --num_thread_reader 8 \\\n","  --epochs 20 \\\n","  --batch_size 64 \\\n","  --train_csv  \"$DATA_PATH\"/MSRVTT_train.subset.csv \\\n","  --val_csv    \"$DATA_PATH\"/MSRVTT_JSFUSION_test.csv \\\n","  --data_path  \"$DATA_PATH\"/MSRVTT_data.json \\\n","  --features_path \"$VIDEO_PATH\" \\\n","  --output_dir /content/drive/MyDrive/CLIP_video_training/CLIP_video/ckpts/ckpt_msrvtt_retrieval_looseType \\\n","  --lr 1e-4 \\\n","  --max_words 32 \\\n","  --max_frames 8 \\\n","  --batch_size_val 8 \\\n","  --datatype msrvtt \\\n","  --expand_msrvtt_sentences \\\n","  --feature_framerate 1 \\\n","  --coef_lr 1e-3 \\\n","  --freeze_layer_num 6 \\\n","  --slice_framepos 2 \\\n","  --loose_type \\\n","  --linear_patch 2d \\\n","  --sim_header meanP \\\n","  --pretrained_clip_name ViT-B/32 \\\n","  --fp16 \\\n","  --fp16_opt_level O1\n"]},{"cell_type":"code","source":[],"metadata":{"id":"D6jYr2zgl-fN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}